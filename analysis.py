import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ƒê·ªçc d·ªØ li·ªáu test set
test_df = pd.read_csv("test_set_all_models.csv")

# L·∫•y danh s√°ch c√°c model
model_names = ['Linear_Regression', 'Random_Forest', 'XGBoost']

# Danh s√°ch features ƒë·ªÉ ph√¢n t√≠ch
features = ['order_item_count', 'delivery_days', 'execution_days',
            'estimated_days', 'avg_review_score', 'total_freight_value',
            'total_payment', 'payment_installments']

print("="*100)
print("üìä PH√ÇN T√çCH CHI TI·∫æT: BEST vs WORST vs TRUNG B√åNH")
print("="*100)

# T√≠nh gi√° tr·ªã trung b√¨nh c·ªßa to√†n b·ªô test set
mean_values = {}
for feat in features:
    if feat in test_df.columns:
        mean_values[feat] = test_df[feat].mean()
mean_values['total_price'] = test_df['total_price'].mean()

print("\nüìà GI√Å TR·ªä TRUNG B√åNH C·ª¶A TO√ÄN B·ªò TEST SET:")
print("-" * 100)
for feat, val in mean_values.items():
    print(f"   {feat:25s}: {val:>15,.2f}")

# Ph√¢n t√≠ch cho t·ª´ng model
all_comparisons = {}

for model_name in model_names:
    pred_col = f'pred_{model_name}'
    error_col = f'error_{model_name}'
    
    # T√≠nh absolute error
    test_df[f'abs_error_{model_name}'] = abs(test_df[error_col])
    
    # T√¨m d·ª± ƒëo√°n ch√≠nh x√°c nh·∫•t
    best_idx = test_df[f'abs_error_{model_name}'].idxmin()
    best_row = test_df.loc[best_idx]
    
    # T√¨m d·ª± ƒëo√°n sai l·ªách nh·∫•t
    worst_idx = test_df[f'abs_error_{model_name}'].idxmax()
    worst_row = test_df.loc[worst_idx]
    
    # L∆∞u ƒë·ªÉ so s√°nh
    all_comparisons[model_name] = {
        'best': best_row,
        'worst': worst_row
    }
    
    # In k·∫øt qu·∫£ chi ti·∫øt
    print(f"\n{'='*100}")
    print(f"ü§ñ MODEL: {model_name.replace('_', ' ').upper()}")
    print(f"{'='*100}")
    
    # T·∫°o b·∫£ng so s√°nh
    comparison_data = []
    
    print(f"\n{'Feature':<25} {'BEST':<20} {'WORST':<20} {'TRUNG B√åNH':<20} {'So s√°nh Best':<25}")
    print("-" * 100)
    
    # Total Price
    best_actual = best_row['total_price']
    best_pred = best_row[pred_col]
    best_error = best_row[error_col]
    best_pct = (abs(best_error) / best_actual) * 100
    
    worst_actual = worst_row['total_price']
    worst_pred = worst_row[pred_col]
    worst_error = worst_row[error_col]
    worst_pct = (abs(worst_error) / worst_actual) * 100
    
    print(f"\n{'TOTAL_PRICE (ACTUAL)':<25} {best_actual:<20,.2f} {worst_actual:<20,.2f} {mean_values['total_price']:<20,.2f}")
    print(f"{'TOTAL_PRICE (PREDICTED)':<25} {best_pred:<20,.2f} {worst_pred:<20,.2f} {'-':<20}")
    print(f"{'ERROR':<25} {best_error:<20,.2f} {worst_error:<20,.2f} {'-':<20}")
    print(f"{'ERROR %':<25} {best_pct:<20.2f}% {worst_pct:<20.2f}% {'-':<20}")
    
    print(f"\n{'--- FEATURES ---':<25}")
    print("-" * 100)
    
    comparison_data = []
    for feat in features:
        if feat in test_df.columns:
            best_val = best_row[feat]
            worst_val = worst_row[feat]
            mean_val = mean_values[feat]
            
            # So s√°nh v·ªõi trung b√¨nh
            best_vs_mean = ((best_val - mean_val) / mean_val * 100) if mean_val != 0 else 0
            worst_vs_mean = ((worst_val - mean_val) / mean_val * 100) if mean_val != 0 else 0
            
            if abs(best_vs_mean) < 10:
                comparison = "‚âà G·∫ßn trung b√¨nh"
            elif best_vs_mean > 0:
                comparison = f"‚Üë Cao h∆°n {best_vs_mean:.1f}%"
            else:
                comparison = f"‚Üì Th·∫•p h∆°n {abs(best_vs_mean):.1f}%"
            
            print(f"{feat:<25} {best_val:<20,.2f} {worst_val:<20,.2f} {mean_val:<20,.2f} {comparison:<25}")
            
            comparison_data.append({
                'Feature': feat,
                'Best': best_val,
                'Worst': worst_val,
                'Mean': mean_val,
                'Best_vs_Mean_%': best_vs_mean,
                'Worst_vs_Mean_%': worst_vs_mean
            })
    
    # PH√ÇN T√çCH NGUY√äN NH√ÇN
    print(f"\n{'='*100}")
    print(f"üîç PH√ÇN T√çCH NGUY√äN NH√ÇN - {model_name.replace('_', ' ').upper()}")
    print(f"{'='*100}")
    
    print(f"\n‚úÖ T·∫†I SAO D·ª∞ ƒêO√ÅN T·ªêT (Sai s·ªë ch·ªâ {best_pct:.2f}%):")
    print("-" * 100)
    near_mean_count = sum(1 for item in comparison_data if abs(item['Best_vs_Mean_%']) < 20)
    print(f"   ‚Ä¢ {near_mean_count}/{len(comparison_data)} features g·∫ßn v·ªõi gi√° tr·ªã trung b√¨nh (¬±20%)")
    print(f"   ‚Ä¢ Gi√° tr·ªã total_price = {best_actual:,.0f} ƒë (Trung b√¨nh: {mean_values['total_price']:,.0f} ƒë)")
    
    # T√¨m features g·∫ßn trung b√¨nh nh·∫•t
    near_mean_features = [item for item in comparison_data if abs(item['Best_vs_Mean_%']) < 20]
    if near_mean_features:
        print(f"   ‚Ä¢ Features n·∫±m trong v√πng 'an to√†n' c·ªßa model:")
        for item in near_mean_features[:3]:
            print(f"      - {item['Feature']}: {item['Best']:.2f} (G·∫ßn {item['Mean']:.2f})")
    
    print(f"\n‚ùå T·∫†I SAO D·ª∞ ƒêO√ÅN T·ªÜ (Sai s·ªë l√™n t·ªõi {worst_pct:.2f}%):")
    print("-" * 100)
    
    # Ki·ªÉm tra outlier
    if worst_actual > test_df['total_price'].quantile(0.95):
        print(f"   ‚Ä¢ ‚ö†Ô∏è  Gi√° tr·ªã total_price = {worst_actual:,.0f} ƒë l√† OUTLIER (cao h∆°n 95% m·∫´u)")
    elif worst_actual < test_df['total_price'].quantile(0.05):
        print(f"   ‚Ä¢ ‚ö†Ô∏è  Gi√° tr·ªã total_price = {worst_actual:,.0f} ƒë l√† OUTLIER (th·∫•p h∆°n 95% m·∫´u)")
    
    # T√¨m features sai l·ªách nhi·ªÅu
    outlier_features = [item for item in comparison_data if abs(item['Worst_vs_Mean_%']) > 50]
    if outlier_features:
        print(f"   ‚Ä¢ ‚ö†Ô∏è  {len(outlier_features)} features b·ªã l·ªách nhi·ªÅu so v·ªõi trung b√¨nh:")
        for item in outlier_features:
            direction = "cao h∆°n" if item['Worst_vs_Mean_%'] > 0 else "th·∫•p h∆°n"
            print(f"      - {item['Feature']}: {item['Worst']:.2f} ({direction} {abs(item['Worst_vs_Mean_%']):.1f}% so v·ªõi TB)")
    
    # Ph√¢n t√≠ch theo t·ª´ng model
    print(f"\n   ‚Ä¢ ƒê·∫∑c ƒëi·ªÉm c·ªßa {model_name.replace('_', ' ')}:")
    if model_name == 'Linear_Regression':
        print(f"      - Gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá TUY·∫æN T√çNH gi·ªØa features v√† target")
        print(f"      - Kh√¥ng x·ª≠ l√Ω t·ªët v·ªõi OUTLIERS v√† t∆∞∆°ng t√°c PHI TUY·∫æN")
        print(f"      - Ho·∫°t ƒë·ªông t·ªët khi d·ªØ li·ªáu g·∫ßn v·ªõi v√πng ƒë√£ h·ªçc (trung b√¨nh)")
    elif model_name == 'Random_Forest':
        print(f"      - D·ª± ƒëo√°n b·∫±ng c√°ch l·∫•y trung b√¨nh c·ªßa nhi·ªÅu decision trees")
        print(f"      - KH√ì NGO·∫†I SUY: Kh√≥ d·ª± ƒëo√°n gi√° tr·ªã n·∫±m ngo√†i ph·∫°m vi training")
        print(f"      - C√≥ th·ªÉ b·ªã OVERFITTING n·∫øu max_depth qu√° l·ªõn")
    elif model_name == 'XGBoost':
        print(f"      - H·ªçc d·∫ßn b·∫±ng c√°ch s·ª≠a l·ªói c·ªßa c√°c c√¢y tr∆∞·ªõc")
        print(f"      - Nh·∫°y c·∫£m v·ªõi OUTLIERS n·∫øu kh√¥ng c√≥ regularization ƒë·ªß")
        print(f"      - C·∫ßn ƒëi·ªÅu ch·ªânh learning_rate v√† max_depth ph√π h·ª£p")

# ==========================================
# VISUALIZATIONS
# ==========================================
print(f"\n{'='*100}")
print("üìä T·∫†O C√ÅC BI·ªÇU ƒê·ªí TR·ª∞C QUAN")
print(f"{'='*100}\n")

# 1. So s√°nh Best vs Worst vs Mean cho t·ª´ng model
fig, axes = plt.subplots(3, 3, figsize=(20, 15))
fig.suptitle('SO S√ÅNH FEATURES: BEST vs WORST vs TRUNG B√åNH', 
             fontsize=16, fontweight='bold', y=0.995)

for model_idx, model_name in enumerate(model_names):
    best_row = all_comparisons[model_name]['best']
    worst_row = all_comparisons[model_name]['worst']
    
    # Ch·ªçn 3 features quan tr·ªçng nh·∫•t ƒë·ªÉ v·∫Ω
    important_features = ['total_payment', 'total_freight_value', 'order_item_count']
    
    for feat_idx, feat in enumerate(important_features):
        if feat in test_df.columns:
            ax = axes[model_idx, feat_idx]
            
            categories = ['BEST\nPrediction', 'WORST\nPrediction', 'MEAN\n(Test Set)']
            values = [best_row[feat], worst_row[feat], mean_values[feat]]
            colors = ['#2ecc71', '#e74c3c', '#3498db']
            
            bars = ax.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)
            
            # Th√™m gi√° tr·ªã l√™n ƒë·∫ßu c·ªôt
            for i, (bar, val) in enumerate(zip(bars, values)):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:,.1f}',
                       ha='center', va='bottom', fontsize=9, fontweight='bold')
            
            ax.set_title(f'{model_name.replace("_", " ")}\n{feat}', 
                        fontsize=11, fontweight='bold')
            ax.set_ylabel('Value', fontsize=9)
            ax.grid(True, alpha=0.3, axis='y', linestyle='--')
            ax.set_axisbelow(True)

plt.tight_layout()
plt.savefig('features_comparison_best_worst_mean.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: features_comparison_best_worst_mean.png")
plt.show()

# 2. Radar Chart so s√°nh t·∫•t c·∫£ features (chu·∫©n h√≥a)
fig, axes = plt.subplots(1, 3, figsize=(20, 6))
fig.suptitle('RADAR CHART: So s√°nh t·∫•t c·∫£ Features (Chu·∫©n h√≥a 0-100)', 
             fontsize=16, fontweight='bold')

for model_idx, model_name in enumerate(model_names):
    ax = axes[model_idx]
    
    best_row = all_comparisons[model_name]['best']
    worst_row = all_comparisons[model_name]['worst']
    
    # Chu·∫©n h√≥a features v·ªÅ 0-100
    normalized_features = []
    feature_names_short = []
    
    for feat in features[:6]:  # Ch·ªâ l·∫•y 6 features ƒë·ªÉ radar chart d·ªÖ nh√¨n
        if feat in test_df.columns:
            min_val = test_df[feat].min()
            max_val = test_df[feat].max()
            
            if max_val - min_val > 0:
                best_norm = (best_row[feat] - min_val) / (max_val - min_val) * 100
                worst_norm = (worst_row[feat] - min_val) / (max_val - min_val) * 100
                mean_norm = (mean_values[feat] - min_val) / (max_val - min_val) * 100
                
                normalized_features.append({
                    'best': best_norm,
                    'worst': worst_norm,
                    'mean': mean_norm
                })
                # R√∫t ng·∫Øn t√™n feature
                short_name = feat.replace('_', ' ').title()[:15]
                feature_names_short.append(short_name)
    
    # V·∫Ω radar chart
    angles = np.linspace(0, 2 * np.pi, len(normalized_features), endpoint=False).tolist()
    angles += angles[:1]  # ƒê√≥ng v√≤ng tr√≤n
    
    best_values = [f['best'] for f in normalized_features]
    worst_values = [f['worst'] for f in normalized_features]
    mean_values_norm = [f['mean'] for f in normalized_features]
    
    best_values += best_values[:1]
    worst_values += worst_values[:1]
    mean_values_norm += mean_values_norm[:1]
    
    ax = plt.subplot(1, 3, model_idx + 1, projection='polar')
    
    ax.plot(angles, best_values, 'o-', linewidth=2, label='Best', color='#2ecc71')
    ax.fill(angles, best_values, alpha=0.15, color='#2ecc71')
    
    ax.plot(angles, worst_values, 'o-', linewidth=2, label='Worst', color='#e74c3c')
    ax.fill(angles, worst_values, alpha=0.15, color='#e74c3c')
    
    ax.plot(angles, mean_values_norm, 'o-', linewidth=2, label='Mean', color='#3498db')
    ax.fill(angles, mean_values_norm, alpha=0.15, color='#3498db')
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(feature_names_short, size=8)
    ax.set_ylim(0, 100)
    ax.set_title(f'{model_name.replace("_", " ")}', size=12, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    ax.grid(True)

plt.tight_layout()
plt.savefig('radar_chart_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: radar_chart_comparison.png")
plt.show()

# 3. Heatmap: % ch√™nh l·ªách so v·ªõi trung b√¨nh
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

for comparison_type, ax in zip(['Best', 'Worst'], axes):
    heatmap_data = []
    
    for model_name in model_names:
        row_data = []
        comp_row = all_comparisons[model_name]['best' if comparison_type == 'Best' else 'worst']
        
        for feat in features:
            if feat in test_df.columns:
                val = comp_row[feat]
                mean_val = mean_values[feat]
                pct_diff = ((val - mean_val) / mean_val * 100) if mean_val != 0 else 0
                row_data.append(pct_diff)
            else:
                row_data.append(0)
        
        heatmap_data.append(row_data)
    
    heatmap_df = pd.DataFrame(
        heatmap_data,
        columns=[f.replace('_', ' ').title()[:15] for f in features],
        index=[m.replace('_', ' ') for m in model_names]
    )
    
    sns.heatmap(heatmap_df, annot=True, fmt='.1f', cmap='RdYlGn', center=0,
                cbar_kws={'label': '% Ch√™nh l·ªách so v·ªõi TB'},
                linewidths=0.5, ax=ax, vmin=-100, vmax=100)
    
    ax.set_title(f'{comparison_type} Predictions: % Ch√™nh l·ªách so v·ªõi Trung b√¨nh', 
                 fontsize=13, fontweight='bold')
    ax.set_xlabel('')
    ax.set_ylabel('Model', fontsize=11)

plt.tight_layout()
plt.savefig('heatmap_deviation_from_mean.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: heatmap_deviation_from_mean.png")
plt.show()

# 4. Bi·ªÉu ƒë·ªì t·ªïng h·ª£p: T·ª∑ l·ªá % error
fig, ax = plt.subplots(figsize=(12, 6))

model_labels = [m.replace('_', ' ') for m in model_names]
best_errors = []
worst_errors = []

for model_name in model_names:
    best_row = all_comparisons[model_name]['best']
    worst_row = all_comparisons[model_name]['worst']
    
    pred_col = f'pred_{model_name}'
    
    best_error_pct = abs(best_row['total_price'] - best_row[pred_col]) / best_row['total_price'] * 100
    worst_error_pct = abs(worst_row['total_price'] - worst_row[pred_col]) / worst_row['total_price'] * 100
    
    best_errors.append(best_error_pct)
    worst_errors.append(worst_error_pct)

x = np.arange(len(model_labels))
width = 0.35

bars1 = ax.bar(x - width/2, best_errors, width, label='Best Prediction Error %', 
               color='#2ecc71', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width/2, worst_errors, width, label='Worst Prediction Error %', 
               color='#e74c3c', alpha=0.8, edgecolor='black')

# Th√™m gi√° tr·ªã l√™n ƒë·∫ßu c·ªôt
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{height:.2f}%',
               ha='center', va='bottom', fontsize=10, fontweight='bold')

ax.set_ylabel('Prediction Error (%)', fontsize=12, fontweight='bold')
ax.set_title('So s√°nh T·ª∑ l·ªá % Sai s·ªë: Best vs Worst Predictions', 
             fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(model_labels, fontsize=11)
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y', linestyle='--')
ax.set_axisbelow(True)

plt.tight_layout()
plt.savefig('error_percentage_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: error_percentage_comparison.png")
plt.show()

print(f"\n{'='*100}")
print("‚úÖ HO√ÄN TH√ÄNH! ƒê√£ t·∫°o 4 bi·ªÉu ƒë·ªì ph√¢n t√≠ch chi ti·∫øt.")
print(f"{'='*100}\n")

print("üìã T√ìM T·∫ÆT K·∫æT LU·∫¨N:")
print("-" * 100)
print("1. ‚úÖ D·ª± ƒëo√°n T·ªêT khi: Features g·∫ßn v·ªõi gi√° tr·ªã trung b√¨nh c·ªßa training set")
print("2. ‚ùå D·ª± ƒëo√°n T·ªÜ khi: G·∫∑p outliers ho·∫∑c gi√° tr·ªã b·∫•t th∆∞·ªùng")
print("3. üéØ Model t·ªët nh·∫•t: Xem bi·ªÉu ƒë·ªì % error ƒë·ªÉ ch·ªçn model c√≥ worst error th·∫•p nh·∫•t")
print("4. üí° C·∫£i thi·ªán: Thu th·∫≠p th√™m d·ªØ li·ªáu outliers ho·∫∑c x·ª≠ l√Ω outliers tr∆∞·ªõc khi train")
print("-" * 100)