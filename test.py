import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from statsmodels.stats.outliers_influence import variance_inflation_factor
import numpy as np

#  ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv("data_cleaning0.csv")

# Gi·ªØ l·∫°i c√°c c·ªôt li√™n quan
features = [
  "delivery_days", "execution_days",
    "estimated_days", "avg_review_score", "total_freight_value",
    "total_payment", "payment_installments"
]
target = "total_price"

# Ki·ªÉm tra c√≥ thi·∫øu d·ªØ li·ªáu kh√¥ng
print("Missing values per column:")
print(df[features + [target]].isnull().sum())

# Lo·∫°i b·ªè d√≤ng thi·∫øu d·ªØ li·ªáu
df = df.dropna(subset=features + [target])

# Ph√¢n t√≠ch t∆∞∆°ng quan
correlations = df[features + [target]].corr()[target].sort_values(ascending=False)
print("\n Correlation with total_price:")
print(correlations)

# Ch·ªçn c√°c feature c√≥ |corr| > 0.1 (c√≥ t∆∞∆°ng quan ƒë√°ng k·ªÉ)
selected_features = [f for f in features if abs(correlations[f]) > 0.1]
print("\n Selected features:", selected_features)

# Ki·ªÉm tra ƒëa c·ªông tuy·∫øn (VIF)
X_vif = df[selected_features].copy()
vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
print("\n VIF (ki·ªÉm tra tr√πng th√¥ng tin gi·ªØa c√°c feature):")
print(vif_data)

# T·∫°o stratification labels
# T·∫°o bins cho target ƒë·ªÉ stratify
df['target_bin'] = pd.qcut(df[target], q=5, labels=False, duplicates='drop')

# T·∫°o composite stratification key t·ª´ nhi·ªÅu features quan tr·ªçng nh·∫•t
# L·∫•y top 3 features c√≥ correlation cao nh·∫•t
top_features = correlations[selected_features].abs().nlargest(min(3, len(selected_features))).index.tolist()

# T·∫°o bins cho t·ª´ng top feature
for feat in top_features:
    df[f'{feat}_bin'] = pd.qcut(df[feat], q=3, labels=False, duplicates='drop')

# K·∫øt h·ª£p c√°c bins th√†nh m·ªôt stratification key
strat_cols = ['target_bin'] + [f'{feat}_bin' for feat in top_features]
df['strat_key'] = df[strat_cols].astype(str).agg('_'.join, axis=1)

strat_counts = df['strat_key'].value_counts()
small_groups = strat_counts[strat_counts < 5].index.tolist()

if not small_groups:
    print("Kh√¥ng t√¨m th·∫•y nh√≥m qu√° nh·ªè n√†o.")
else:

    # T·∫°o danh s√°ch c√°c nh√≥m h·ª£p l·ªá (ƒë·ªß l·ªõn)
    valid_groups = strat_counts[strat_counts >= 5].index.tolist()

    for small_key in small_groups:
        # 1. T√°ch ra target_bin c·ªßa nh√≥m l·ªói
        target_bin_of_small_group = small_key.split('_')[0]

        # 2. T√¨m c√°c nh√≥m h·ª£p l·ªá (valid_groups) c√≥ c√πng target_bin
        #    S·ª≠ d·ª•ng 'startswith' ƒë·ªÉ l·ªçc c√°c key
        candidates = [
            key for key in valid_groups 
            if key.startswith(target_bin_of_small_group + '_')
        ]
        
        if candidates:
            # 3. Ch·ªçn nh√≥m l·ªõn nh·∫•t trong s·ªë c√°c ·ª©ng c·ª≠ vi√™n
            best_merge_key = max(candidates, key=lambda k: strat_counts[k])
            
            # G√°n l·∫°i c√°c m·∫´u l·ªói v√†o nh√≥m t·ªëi ∆∞u
            df.loc[df['strat_key'] == small_key, 'strat_key'] = best_merge_key
        else:
            largest_group_key = strat_counts.idxmax()
            df.loc[df['strat_key'] == small_key, 'strat_key'] = largest_group_key

X = df[selected_features]
y = df[target]

try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=df['strat_key']
    )
    print("\n S·ª≠ d·ª•ng stratified sampling")
except ValueError:
    # N·∫øu c√≥ nh√≥m qu√° nh·ªè, fallback v·ªÅ random split
    print("\n M·ªôt s·ªë nh√≥m qu√° nh·ªè, s·ª≠ d·ª•ng random sampling")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

print(f"\nTrain size: {len(X_train)}, Test size: {len(X_test)}")

# So s√°nh ph√¢n ph·ªëi gi·ªØa train v√† test
print("\n So s√°nh ph√¢n ph·ªëi Train vs Test:")
print("\nTarget (total_price):")
print(f"Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}, Min: {y_train.min():.2f}, Max: {y_train.max():.2f}")
print(f"Test  - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}, Min: {y_test.min():.2f}, Max: {y_test.max():.2f}")

print("\nTop Features:")
for feat in top_features[:3]:
    print(f"\n{feat}:")
    print(f"Train - Mean: {X_train[feat].mean():.2f}, Std: {X_train[feat].std():.2f}")
    print(f"Test  - Mean: {X_test[feat].mean():.2f}, Std: {X_test[feat].std():.2f}")

# ===============================
# 6Ô∏è‚É£ Hu·∫•n luy·ªán m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh
# ===============================
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("\n Evaluation:")
print(f"R¬≤: {r2:.4f}")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")

# ===============================
# 8Ô∏è‚É£ Xem tr·ªçng s·ªë (h·ªá s·ªë h·ªìi quy)
# ===============================
coef_df = pd.DataFrame({
    "Feature": selected_features,
    "Coefficient": model.coef_
}).sort_values(by="Coefficient", ascending=False)

print("\nüîç Feature Importance (Linear Regression Coefficients):")
print(coef_df)

# ===============================
# 9Ô∏è‚É£ Xu·∫•t t·∫≠p train v√† test ra file
# ===============================
# T·∫°o DataFrame train
train_df = X_train.copy()
train_df[target] = y_train
train_df.to_csv("train_set1.csv", index=False)

# T·∫°o DataFrame test (c√≥ c·∫£ gi√° tr·ªã th·ª±c v√† d·ª± ƒëo√°n)
test_df = X_test.copy()
test_df[target] = y_test
test_df['predicted_' + target] = y_pred
test_df['prediction_error'] = y_test.values - y_pred
test_df.to_csv("test_set1.csv", index=False)


import matplotlib.pyplot as plt

plt.scatter(y_test, model.predict(X_test))
plt.xlabel("Actual Revenue")
plt.ylabel("Predicted Revenue")
plt.title("Actual vs Predicted Revenue")
plt.show()


print("\n" + "="*60)