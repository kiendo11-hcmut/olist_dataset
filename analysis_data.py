import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ƒê·ªçc d·ªØ li·ªáu test set
test_df = pd.read_csv("test_set_all_models.csv")

# L·∫•y danh s√°ch c√°c model
model_names = ['Linear_Regression', 'Random_Forest', 'XGBoost']

print("="*80)
print("üìä PH√ÇN T√çCH K·∫æT QU·∫¢ CH√çNH X√ÅC NH·∫§T V√Ä SAI L·ªÜCH NH·∫§T")
print("="*80)

# Ph√¢n t√≠ch cho t·ª´ng model
analysis_results = []

for model_name in model_names:
    pred_col = f'pred_{model_name}'
    error_col = f'error_{model_name}'
    
    # T√≠nh absolute error
    test_df[f'abs_error_{model_name}'] = abs(test_df[error_col])
    
    # T√¨m d·ª± ƒëo√°n ch√≠nh x√°c nh·∫•t (error nh·ªè nh·∫•t)
    best_idx = test_df[f'abs_error_{model_name}'].idxmin()
    best_actual = test_df.loc[best_idx, 'total_price']
    best_pred = test_df.loc[best_idx, pred_col]
    best_error = test_df.loc[best_idx, error_col]
    best_pct_error = (abs(best_error) / best_actual) * 100
    
    # T√¨m d·ª± ƒëo√°n sai l·ªách nh·∫•t (error l·ªõn nh·∫•t)
    worst_idx = test_df[f'abs_error_{model_name}'].idxmax()
    worst_actual = test_df.loc[worst_idx, 'total_price']
    worst_pred = test_df.loc[worst_idx, pred_col]
    worst_error = test_df.loc[worst_idx, error_col]
    worst_pct_error = (abs(worst_error) / worst_actual) * 100
    
    # L∆∞u th√¥ng tin chi ti·∫øt
    analysis_results.append({
        'Model': model_name.replace('_', ' '),
        'Best_Actual': best_actual,
        'Best_Pred': best_pred,
        'Best_Error': best_error,
        'Best_%Error': best_pct_error,
        'Worst_Actual': worst_actual,
        'Worst_Pred': worst_pred,
        'Worst_Error': worst_error,
        'Worst_%Error': worst_pct_error,
        'Best_Features': test_df.loc[best_idx].to_dict(),
        'Worst_Features': test_df.loc[worst_idx].to_dict()
    })
    
    # In k·∫øt qu·∫£
    print(f"\n{'='*80}")
    print(f"üîç {model_name.replace('_', ' ').upper()}")
    print(f"{'='*80}")
    
    print(f"\n‚úÖ D·ª∞ ƒêO√ÅN CH√çNH X√ÅC NH·∫§T:")
    print(f"   ‚Ä¢ Gi√° tr·ªã th·ª±c t·∫ø: {best_actual:,.2f}")
    print(f"   ‚Ä¢ Gi√° tr·ªã d·ª± ƒëo√°n: {best_pred:,.2f}")
    print(f"   ‚Ä¢ Sai s·ªë: {best_error:,.2f} ({best_pct_error:.2f}%)")
    print(f"\n   üìã ƒê·∫∑c ƒëi·ªÉm c·ªßa giao d·ªãch n√†y:")
    for feat in ['order_item_count', 'delivery_days', 'execution_days', 
                 'estimated_days', 'avg_review_score', 'total_freight_value',
                 'total_payment', 'payment_installments']:
        if feat in test_df.columns:
            print(f"      - {feat}: {test_df.loc[best_idx, feat]:.2f}")
    
    print(f"\n‚ùå D·ª∞ ƒêO√ÅN SAI L·ªÜCH NH·∫§T:")
    print(f"   ‚Ä¢ Gi√° tr·ªã th·ª±c t·∫ø: {worst_actual:,.2f}")
    print(f"   ‚Ä¢ Gi√° tr·ªã d·ª± ƒëo√°n: {worst_pred:,.2f}")
    print(f"   ‚Ä¢ Sai s·ªë: {worst_error:,.2f} ({worst_pct_error:.2f}%)")
    print(f"   ‚Ä¢ H∆∞·ªõng sai l·ªách: {'D·ª∞ ƒêO√ÅN CAO H∆†N' if worst_error < 0 else 'D·ª∞ ƒêO√ÅN TH·∫§P H∆†N'}")
    print(f"\n   üìã ƒê·∫∑c ƒëi·ªÉm c·ªßa giao d·ªãch n√†y:")
    for feat in ['order_item_count', 'delivery_days', 'execution_days',
                 'estimated_days', 'avg_review_score', 'total_freight_value',
                 'total_payment', 'payment_installments']:
        if feat in test_df.columns:
            print(f"      - {feat}: {test_df.loc[worst_idx, feat]:.2f}")

# T·∫°o b·∫£ng so s√°nh
print(f"\n{'='*80}")
print("üìà B·∫¢NG SO S√ÅNH T√ìM T·∫ÆT")
print(f"{'='*80}\n")

comparison_df = pd.DataFrame([
    {
        'Model': r['Model'],
        'Best Error (%)': f"{r['Best_%Error']:.2f}%",
        'Worst Error (%)': f"{r['Worst_%Error']:.2f}%",
        'Error Range': f"{r['Worst_%Error'] - r['Best_%Error']:.2f}%"
    }
    for r in analysis_results
])
print(comparison_df.to_string(index=False))

# PH√ÇN T√çCH T·∫†I SAO
print(f"\n{'='*80}")
print("üî¨ PH√ÇN T√çCH NGUY√äN NH√ÇN")
print(f"{'='*80}\n")

for result in analysis_results:
    model_name = result['Model']
    print(f"\nüìå {model_name.upper()}:")
    
    # Ph√¢n t√≠ch d·ª± ƒëo√°n t·ªët
    print(f"\n   ‚úÖ D·ª± ƒëo√°n ch√≠nh x√°c v√¨:")
    best_feat = result['Best_Features']
    print(f"      ‚Ä¢ Gi√° tr·ªã n·∫±m g·∫ßn trung b√¨nh c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán")
    print(f"      ‚Ä¢ Kh√¥ng c√≥ gi√° tr·ªã b·∫•t th∆∞·ªùng (outlier)")
    print(f"      ‚Ä¢ C√°c features c√≥ m·ªëi t∆∞∆°ng quan m·∫°nh v·ªõi target")
    
    # Ph√¢n t√≠ch d·ª± ƒëo√°n k√©m
    print(f"\n   ‚ùå D·ª± ƒëo√°n sai l·ªách v√¨:")
    worst_feat = result['Worst_Features']
    
    # Ki·ªÉm tra c√°c y·∫øu t·ªë
    if worst_feat.get('total_price', 0) > test_df['total_price'].quantile(0.95):
        print(f"      ‚Ä¢ Gi√° tr·ªã thu·ªôc nh√≥m OUTLIER (cao h∆°n 95% d·ªØ li·ªáu)")
    elif worst_feat.get('total_price', 0) < test_df['total_price'].quantile(0.05):
        print(f"      ‚Ä¢ Gi√° tr·ªã thu·ªôc nh√≥m OUTLIER (th·∫•p h∆°n 95% d·ªØ li·ªáu)")
    
    if model_name == 'Linear Regression':
        print(f"      ‚Ä¢ Linear Regression gi·∫£ ƒë·ªãnh m·ªëi quan h·ªá tuy·∫øn t√≠nh")
        print(f"      ‚Ä¢ Kh√¥ng x·ª≠ l√Ω t·ªët c√°c t∆∞∆°ng t√°c phi tuy·∫øn gi·ªØa features")
        print(f"      ‚Ä¢ Nh·∫°y c·∫£m v·ªõi outliers v√† multicollinearity")
    elif model_name == 'Random Forest':
        print(f"      ‚Ä¢ Random Forest c√≥ th·ªÉ b·ªã overfitting v·ªõi d·ªØ li·ªáu ph·ª©c t·∫°p")
        print(f"      ‚Ä¢ Kh√≥ d·ª± ƒëo√°n gi√° tr·ªã n·∫±m ngo√†i ph·∫°m vi training data")
        print(f"      ‚Ä¢ C√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh hyperparameters (max_depth, min_samples)")
    elif model_name == 'XGBoost':
        print(f"      ‚Ä¢ XGBoost nh·∫°y c·∫£m v·ªõi outliers trong target variable")
        print(f"      ‚Ä¢ Learning rate c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh")
        print(f"      ‚Ä¢ C√≥ th·ªÉ c·∫ßn th√™m regularization (reg_alpha, reg_lambda)")

# Visualizations
print(f"\n{'='*80}")
print("üìä T·∫†O BI·ªÇU ƒê·ªí TR·ª∞C QUAN")
print(f"{'='*80}\n")

# 1. Bi·ªÉu ƒë·ªì ph√¢n b·ªë error c·ªßa t·ª´ng model
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for idx, model_name in enumerate(model_names):
    error_col = f'error_{model_name}'
    axes[idx].hist(test_df[error_col], bins=50, alpha=0.7, color=['blue', 'green', 'orange'][idx])
    axes[idx].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[idx].set_title(f'{model_name.replace("_", " ")}\nError Distribution')
    axes[idx].set_xlabel('Prediction Error')
    axes[idx].set_ylabel('Frequency')
    axes[idx].grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('error_distribution_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: error_distribution_comparison.png")
plt.show()

# 2. Bi·ªÉu ƒë·ªì so s√°nh best vs worst predictions
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
for idx, (model_name, result) in enumerate(zip(model_names, analysis_results)):
    categories = ['Best\nPrediction', 'Worst\nPrediction']
    actual = [result['Best_Actual'], result['Worst_Actual']]
    predicted = [result['Best_Pred'], result['Worst_Pred']]
    
    x = np.arange(len(categories))
    width = 0.35
    
    axes[idx].bar(x - width/2, actual, width, label='Actual', alpha=0.8)
    axes[idx].bar(x + width/2, predicted, width, label='Predicted', alpha=0.8)
    axes[idx].set_title(f'{model_name.replace("_", " ")}')
    axes[idx].set_xticks(x)
    axes[idx].set_xticklabels(categories)
    axes[idx].set_ylabel('Total Price')
    axes[idx].legend()
    axes[idx].grid(True, alpha=0.3, axis='y')
    
    # Th√™m % error
    for i, cat in enumerate(categories):
        if cat.startswith('Best'):
            pct = result['Best_%Error']
        else:
            pct = result['Worst_%Error']
        axes[idx].text(i, max(actual[i], predicted[i]) * 1.05, 
                      f'{pct:.1f}%', ha='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('best_worst_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: best_worst_comparison.png")
plt.show()

# 3. Heatmap so s√°nh % error
fig, ax = plt.subplots(figsize=(10, 6))
error_matrix = []
for result in analysis_results:
    error_matrix.append([result['Best_%Error'], result['Worst_%Error']])

sns.heatmap(error_matrix, 
            annot=True, 
            fmt='.2f',
            cmap='RdYlGn_r',
            xticklabels=['Best Prediction Error (%)', 'Worst Prediction Error (%)'],
            yticklabels=[r['Model'] for r in analysis_results],
            cbar_kws={'label': 'Percentage Error'},
            ax=ax)
ax.set_title('Heatmap: Percentage Error Comparison Across Models', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('error_heatmap.png', dpi=300, bbox_inches='tight')
print("‚úì ƒê√£ l∆∞u: error_heatmap.png")
plt.show()

print(f"\n{'='*80}")
print("‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH!")
print(f"{'='*80}\n")
